{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal dependencies with __[Stanza](https://stanfordnlp.github.io/stanza/#getting-started)__ and __[spacy_conll](https://spacy.io/universe/project/spacy-conll)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import razdel\n",
    "import stanza\n",
    "import six\n",
    "from spacy_conll import init_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_filepath = '/Users/oksana/Dev/data/october_final.csv'\n",
    "news = pd.read_csv(news_filepath, dtype={'hash':'uint64'} )\n",
    "# news['hash'] = pd.util.hash_pandas_object(news.link)\n",
    "\n",
    "news['title'] = news.title.str.replace(r'\\n', ' ')\n",
    "news['all_text'] = news.title.str.cat(news.text, sep='\\n', na_rep = '')\n",
    "news['all_text'] = news.all_text.str.strip()\n",
    "\n",
    "news['all_text'] = news['all_text'].str.replace('не очень', 'неочень')\n",
    "news['all_text'] = news['all_text'].str.replace('не дуже', 'недуже')\n",
    "\n",
    "news.all_text = news.all_text.str.replace(r'^Редактор Цензор\\.НЕТ\\n', '', flags=re.M)\n",
    "news.all_text = news.all_text.str.replace(r'Цензор\\.НЕТ', 'Цензор')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 5.12MB/s]                    \n",
      "2021-01-15 13:35:57 INFO: Downloading default packages for language: uk (Ukrainian)...\n",
      "2021-01-15 13:35:58 INFO: File exists: /Users/oksana/stanza_resources/uk/default.zip.\n",
      "2021-01-15 13:36:03 INFO: Finished downloading models and saved to /Users/oksana/stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 9.84MB/s]                    \n",
      "2021-01-15 13:36:03 INFO: Downloading these customized packages for language: ru (Russian)...\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| pretrain  | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-01-15 13:36:03 INFO: File exists: /Users/oksana/stanza_resources/ru/tokenize/gsd.pt.\n",
      "2021-01-15 13:36:03 INFO: File exists: /Users/oksana/stanza_resources/ru/pos/gsd.pt.\n",
      "2021-01-15 13:36:03 INFO: File exists: /Users/oksana/stanza_resources/ru/lemma/gsd.pt.\n",
      "2021-01-15 13:36:03 INFO: File exists: /Users/oksana/stanza_resources/ru/depparse/gsd.pt.\n",
      "2021-01-15 13:36:04 INFO: File exists: /Users/oksana/stanza_resources/ru/pretrain/gsd.pt.\n",
      "2021-01-15 13:36:04 INFO: Finished downloading models and saved to /Users/oksana/stanza_resources.\n",
      "2021-01-15 13:36:04 INFO: Loading these models for language: uk (Ukrainian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | iu      |\n",
      "| lemma     | iu      |\n",
      "=======================\n",
      "\n",
      "2021-01-15 13:36:04 INFO: Use device: cpu\n",
      "2021-01-15 13:36:04 INFO: Loading: tokenize\n",
      "2021-01-15 13:36:05 INFO: Loading: lemma\n",
      "2021-01-15 13:36:05 INFO: Done loading processors!\n",
      "2021-01-15 13:36:05 INFO: Loading these models for language: uk (Ukrainian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | iu      |\n",
      "| pos       | iu      |\n",
      "| lemma     | iu      |\n",
      "| depparse  | iu      |\n",
      "=======================\n",
      "\n",
      "2021-01-15 13:36:05 INFO: Use device: cpu\n",
      "2021-01-15 13:36:05 INFO: Loading: tokenize\n",
      "2021-01-15 13:36:05 INFO: Loading: pos\n",
      "2021-01-15 13:36:07 INFO: Loading: lemma\n",
      "2021-01-15 13:36:07 INFO: Loading: depparse\n",
      "2021-01-15 13:36:09 INFO: Done loading processors!\n",
      "2021-01-15 13:36:10 INFO: Loading these models for language: ru (Russian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-01-15 13:36:10 INFO: Use device: cpu\n",
      "2021-01-15 13:36:10 INFO: Loading: tokenize\n",
      "2021-01-15 13:36:10 INFO: Loading: pos\n",
      "2021-01-15 13:36:11 INFO: Loading: lemma\n",
      "2021-01-15 13:36:12 INFO: Loading: depparse\n",
      "2021-01-15 13:36:14 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('uk')\n",
    "stanza.download('ru', package='gsd', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "nlp = stanza.Pipeline('uk', processors='tokenize,lemma')\n",
    "\n",
    "nlp_uk = init_parser(\n",
    "        \"stanza\",\n",
    "        \"uk\", \n",
    "        is_tokenized = True,\n",
    "        include_headers=False,\n",
    "        parser_opts = {'processors': 'tokenize,pos,lemma,depparse'}\n",
    "    )\n",
    "\n",
    "nlp_ru = init_parser(\n",
    "        \"stanza\",\n",
    "        \"ru\", \n",
    "        is_tokenized = True,\n",
    "        include_headers=False,\n",
    "        parser_opts = {'package': 'gsd', 'processors': 'tokenize,pos,lemma,depparse'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "МОЗ оновило коронавірусні списки країн: Сім держав перейшли в зелену зону \n",
      "\n",
      "1\tМОЗ\tМОЗ\tPROPN\tY\t_\t2\tnsubj\t_\t_\n",
      "2\tоновило\tоновити\tVERB\tVmeis-sn\t_\t0\troot\t_\t_\n",
      "3\tкоронавірусні\tкоронавірусний\tADJ\tAo--pasn\t_\t4\tamod\t_\t_\n",
      "4\tсписки\tсписок\tNOUN\tNcmpan\t_\t2\tobj\t_\t_\n",
      "5\tкраїн:\tкраїна\tNOUN\tNcfsgn\t_\t4\tnmod\t_\t_\n",
      "6\tСім\tсім\tNOUN\tNcfpgn\t_\t5\tnmod\t_\t_\n",
      "7\tдержав\tдержава\tNOUN\tNcfpgn\t_\t6\tnmod\t_\t_\n",
      "8\tперейшли\tперейти\tVERB\tVmeis-p\t_\t2\tconj\t_\t_\n",
      "9\tв\tв\tADP\tSpsa\t_\t11\tcase\t_\t_\n",
      "10\tзелену\tзелений\tADJ\tAfpfsas\t_\t11\tamod\t_\t_\n",
      "11\tзону\tзона\tNOUN\tNcfsan\t_\t8\tobl\t_\tSpaceAfter=No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news.title.iloc[10], '\\n')\n",
    "doc = nlp_uk(news.title.iloc[10])\n",
    "conll = doc._.conll_str\n",
    "print(conll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCENT = six.unichr(769)\n",
    "WORD_TOKENIZATION_RULES = re.compile(r\"\"\"\n",
    "[\\w\"\"\" + ACCENT + \"\"\"]+://(?:[a-zA-Z]|[0-9]|[$-_@.&+])+\n",
    "|[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+\n",
    "|[0-9]+-[а-яА-ЯіїєґІЇҐЄёЁ'’`\"\"\" + ACCENT + \"\"\"]+\n",
    "|[+-]?[0-9](?:[0-9,.-]*[0-9])?\n",
    "|[\\w\"\"\" + ACCENT + \"\"\"](?:[\\w'’`-\"\"\" + ACCENT + \"\"\"]?[\\w\"\"\" + ACCENT + \"\"\"]+)*\n",
    "|[\\w\"\"\" + ACCENT + \"\"\"].(?:\\[\\w\"\"\" + ACCENT + \"\"\"].)+[\\w\"\"\" + ACCENT + \"\"\"]?\n",
    "|[\"#$%&*+,/:;<=>@^`~…\\\\(\\\\)⟨⟩{}\\[\\|\\]‒–—―«»“”‘’'№]\n",
    "|[.!?]+\n",
    "|-+\n",
    "\"\"\", re.X | re.U)\n",
    "\n",
    "ABBRS = \"\"\"\n",
    "ім.\n",
    "в.\n",
    "о.\n",
    "т.\n",
    "п.\n",
    "д.\n",
    "под.\n",
    "ін.\n",
    "вул.\n",
    "просп.\n",
    "бул.\n",
    "пров.\n",
    "пл.\n",
    "г.\n",
    "р.\n",
    "див.\n",
    "п.\n",
    "с.\n",
    "м.\n",
    "\"\"\".strip().split()\n",
    "\n",
    "\n",
    "def tokenize_sents(string):\n",
    "    string = six.text_type(string)\n",
    "    spans = []\n",
    "    for match in re.finditer('[^\\s]+', string):\n",
    "        spans.append(match)\n",
    "    spans_count = len(spans)\n",
    "\n",
    "    rez = []\n",
    "    off = 0\n",
    "\n",
    "    for i in range(spans_count):\n",
    "        tok = string[spans[i].start():spans[i].end()]\n",
    "        if i == spans_count - 1:\n",
    "            rez.append(string[off:spans[i].end()])\n",
    "        elif tok[-1] in ['.', '!', '?', '…', '»', \"'\", \"\\\"\"]:\n",
    "            # tok1 = tok[re.search('[.!?…»]', tok).start() - 1]\n",
    "            next_tok = string[spans[i + 1].start():spans[i + 1].end()]\n",
    "            if (next_tok[0].isupper() or next_tok[0] in [\"'\", \"\\\"\", \"«\"]) \\\n",
    "                    and not ((len(tok) == 2 and tok[0].isupper()) \\\n",
    "                             or tok[0] == '('\n",
    "                             or tok in ABBRS):\n",
    "                rez.append(string[off:spans[i].end()])\n",
    "                off = spans[i + 1].start()\n",
    "\n",
    "    return rez\n",
    "\n",
    "\n",
    "def text_to_sent(text, lang):\n",
    "    rez = []\n",
    "    if lang == 'uk':\n",
    "        for part in text.split('\\n'):\n",
    "            rez += tokenize_sents(part)\n",
    "    elif lang=='ru':\n",
    "        for part in text.split('\\n'):\n",
    "            rez += [s.text for s in razdel.sentenize(part)]\n",
    "    return rez\n",
    "\n",
    "\n",
    "def sent_to_words(text, lang):\n",
    "    if lang == 'uk':\n",
    "        return re.findall(WORD_TOKENIZATION_RULES, text)\n",
    "    elif lang == 'ru':\n",
    "        return [tkn.text for tkn in razdel.tokenize(text)]\n",
    "    return None\n",
    "\n",
    "\n",
    "def tokenize(text, lang):\n",
    "    res = []\n",
    "    for sent in text_to_sent(text, lang):\n",
    "        tokens = []\n",
    "        for word in sent_to_words(sent, lang):\n",
    "            tokens.append(word)\n",
    "        res.append(' '.join(tokens))\n",
    "    return '\\n'.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "МОЗ оновило коронавірусні списки країн : Сім держав перейшли в зелену зону\n",
      "Ілюстроване\n",
      "МОЗ відносить до червоного списку ті країни , де захворюваність на 100 тис . населення вище , ніж в Україні\n",
      "Міністерство охорони здоров'я України оновило червоний коронавірусний список країн .\n",
      "Зокрема , в зелену зону перейшли Угорщина , Чилі , ОАЕ , Катар , Боснія і Герцеговина , Монако , Тринідад і Тобаго .\n",
      "Про це свідчать дані МОЗ .\n",
      "Як повідомляється , МОЗ при перегляді списків відносить до \" червоного \" ті держави , де захворюваність на 100 тисяч населення за останні два тижні вище , ніж в Україні , зараз цей показник виріс зі 109,5 до 126,5 за сім днів .\n",
      "Так , станом на 2 жовтня , до країн червоної зони віднесені відкриті для українських туристів США , Мальдівські та Багамські острови , Ірак , Бразилія , Чорногорія , а також закриті для українських туристів Молдова , Іспанія , Франція , Чехія , Ізраїль .\n",
      "У зелений список увійшли Албанія , Вірменія , Велика Британія , Сербія , Північна Македонія , Ірландія , Іран , Туреччина , Єгипет , Казахстан , Словенія , Білорусь , Сейшельські острови , Коморські острови , Кенія , Мексика , а також закриті для українських туристів Росія , Італія , Канада , Грузія , Польща , Китай , Німеччина , Кіпр і Туніс .\n",
      "МОЗ\n",
      "Нагадаємо , станом на ранок 5 жовтня у світі виявлено 35 157 350 випадків коронавірусу , з них 24 505 898 осіб видужали , 1 037 075 — померли .\n",
      "Тим часом в Україні виявлено 230 236 випадків захворювання коронавірусом , з них 4 430 летальних випадків , а 101 252 пацієнта одужали .\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(news.all_text.iloc[10], news.language.iloc[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 6s, sys: 7.12 s, total: 4min 13s\n",
      "Wall time: 4min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "news['tokenized'] = news.apply(lambda row: tokenize(row.all_text, row.language), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_conllu(doc, link_hash, out_dir = '/Users/oksana/Dev/LDA_SentimentPipeline/test/conllu/'):\n",
    "    out_file = out_dir + \"{}.conll\".format(link_hash)\n",
    "    for sent_idx, sent in enumerate(doc.sents, 1):\n",
    "        header = ['### ', sent_idx, link_hash]\n",
    "        pd.DataFrame([header]).to_csv(out_file, sep='\\t', index=False, header=None, mode='a')\n",
    "        sent._.conll_pd.to_csv(out_file, index=False, sep=\"\\t\", encoding='utf-8', mode='a', header=None)\n",
    "        with open(out_file, 'a') as f:\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_uk = news[news.language=='uk'].copy()\n",
    "news_ru = news[news.language=='ru'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news(news, nlp=nlp_uk, start=0, finish=1000, step=100):\n",
    "    for k in range(start, finish, step):\n",
    "        try:\n",
    "            del news_part\n",
    "        except:\n",
    "            pass    \n",
    "        news_part = news.iloc[k:k + step].copy()\n",
    "        news_part['docs'] = news_part.tokenized.apply(nlp)\n",
    "        news_part.apply(lambda row: save_to_conllu(row.docs, row.hash), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.4 s, sys: 2.48 s, total: 21.8 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_news(news_uk, nlp_uk, start=0, finish=10, step=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis with __[UUUSA](https://github.com/aghie/uuusa)__   ( __[article](https://arxiv.org/abs/1606.05545)__,  __[manual](http://grupolys.org/software/UUUSA/uuusa-user-manual.pdf)__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java -Dfile.encoding=UTF-8 -jar -Xmx2g samulan-0.1.0.jar \\ \n",
    "-s UkSentiData \\\n",
    "-r configuration_uk.xml \\\n",
    "-c parsed.conll \\\n",
    "-p samulan.properties \\\n",
    "-v true\n",
    "-o output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in *.conll; do java -Dfile.encoding=UTF-8 -jar -Xmx2g ../samulan-0.1.1.jar \\\n",
    "-s ../UkSentiData \\\n",
    "-r ../configuration_uk.xml \\\n",
    "-c $file \\\n",
    "-p ../samulan.properties\\\n",
    "-sc so \\\n",
    "-o ../output/$file; done"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.0\t\tКаменських назвала свій головний привід для радості , і це не Потап : \"\"\"\" Згодні ? \"\"\"\"\t [The analysis took: 0.060065182 seg.] [Accumulated time is: 0.060065182]\n",
    "3.0\t\tПопулярна українська співачка Настя Каменських , яка минулого року вийшла заміж за свого колегу Олексія Потапенка , часто радує публіку яскравими постами в своєму Instagram .\t [The analysis took: 0.001861094 seg.] [Accumulated time is: 0.061926276]\n",
    "1.0\t\tНа карантині у зірки з'явилося більше часу для спілкування з фанатами , чим вона активно користується .\t [The analysis took: 0.001455214 seg.] [Accumulated time is: 0.06338149]\n",
    "0.0\t\tЧитайте Знай в Google News !\t [The analysis took: 7.0612E-4 seg.] [Accumulated time is: 0.06408761]\n",
    "2.0\t\tНещодавно дружина Потапа розмістила свіже фото , на якому позує біля пальми в легкому жіночному образі – жовтих шортах , білій сорочці та верху від купальника теж білого кольору .\t [The analysis took: 0.002728221 seg.] [Accumulated time is: 0.066815831]\n",
    "1.0\t\tУ підписі Каменських зазначила , що п'ятниця вже є приводом для радості .\t [The analysis took: 0.002728221 seg.] [Accumulated time is: 0.066815831]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_files = os.listdir(sentiment_dir)\n",
    "res = []\n",
    "for file in sentiment_files:\n",
    "    s = pd.read_csv(file, sep='\\t', usecols=[0], header=None)[0]\n",
    "    res.append((file.strip('.conll'), s.sum(), s.astype(str).str.cat(sep=';')))\n",
    "df = pd.DataFrame(res, columns=['hash', 'sentiment', 'sent_list'])\n",
    "df.hash = df.hash.astype('uint64')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
