{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal dependencies with __[Stanza](https://stanfordnlp.github.io/stanza/#getting-started)__ and __[spacy_conll](https://spacy.io/universe/project/spacy-conll)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import razdel\n",
    "import stanza\n",
    "import six\n",
    "from spacy_conll import init_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oksana/Dev/TextClassification/venv/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "/Users/oksana/Dev/TextClassification/venv/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  del sys.path[0]\n",
      "/Users/oksana/Dev/TextClassification/venv/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "news_filepath = '/Users/oksana/Dev/data/april.csv'\n",
    "news = pd.read_csv(news_filepath)\n",
    "#                    , dtype={'hash':'uint64'} )\n",
    "news['hash'] = pd.util.hash_pandas_object(news.link)\n",
    "\n",
    "news['title'] = news.title.str.replace(r'\\n', ' ')\n",
    "news['all_text'] = news.title.str.cat(news.text, sep='\\n', na_rep = '')\n",
    "news['all_text'] = news.all_text.str.strip()\n",
    "\n",
    "news['all_text'] = news['all_text'].str.replace('не очень', 'неочень')\n",
    "news['all_text'] = news['all_text'].str.replace('не дуже', 'недуже')\n",
    "\n",
    "news.all_text = news.all_text.str.replace(r'^Редактор Цензор\\.НЕТ\\n', '', flags=re.M)\n",
    "news.all_text = news.all_text.str.replace(r'Цензор\\.НЕТ', 'Цензор')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 7.87MB/s]                    \n",
      "2021-06-07 20:34:09 INFO: Downloading default packages for language: uk (Ukrainian)...\n",
      "2021-06-07 20:34:10 INFO: File exists: /Users/oksana/stanza_resources/uk/default.zip.\n",
      "2021-06-07 20:34:15 INFO: Finished downloading models and saved to /Users/oksana/stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 11.1MB/s]                    \n",
      "2021-06-07 20:34:15 INFO: Downloading these customized packages for language: ru (Russian)...\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| pretrain  | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-06-07 20:34:15 INFO: File exists: /Users/oksana/stanza_resources/ru/tokenize/gsd.pt.\n",
      "2021-06-07 20:34:15 INFO: File exists: /Users/oksana/stanza_resources/ru/pos/gsd.pt.\n",
      "2021-06-07 20:34:15 INFO: File exists: /Users/oksana/stanza_resources/ru/lemma/gsd.pt.\n",
      "2021-06-07 20:34:16 INFO: File exists: /Users/oksana/stanza_resources/ru/depparse/gsd.pt.\n",
      "2021-06-07 20:34:17 INFO: File exists: /Users/oksana/stanza_resources/ru/pretrain/gsd.pt.\n",
      "2021-06-07 20:34:17 INFO: Finished downloading models and saved to /Users/oksana/stanza_resources.\n",
      "2021-06-07 20:34:17 INFO: Loading these models for language: uk (Ukrainian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | iu      |\n",
      "| lemma     | iu      |\n",
      "=======================\n",
      "\n",
      "2021-06-07 20:34:17 INFO: Use device: cpu\n",
      "2021-06-07 20:34:17 INFO: Loading: tokenize\n",
      "2021-06-07 20:34:17 INFO: Loading: lemma\n",
      "2021-06-07 20:34:17 INFO: Done loading processors!\n",
      "2021-06-07 20:34:17 INFO: Loading these models for language: uk (Ukrainian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | iu      |\n",
      "| pos       | iu      |\n",
      "| lemma     | iu      |\n",
      "| depparse  | iu      |\n",
      "=======================\n",
      "\n",
      "2021-06-07 20:34:17 INFO: Use device: cpu\n",
      "2021-06-07 20:34:17 INFO: Loading: tokenize\n",
      "2021-06-07 20:34:17 INFO: Loading: pos\n",
      "2021-06-07 20:34:19 INFO: Loading: lemma\n",
      "2021-06-07 20:34:19 INFO: Loading: depparse\n",
      "2021-06-07 20:34:21 INFO: Done loading processors!\n",
      "2021-06-07 20:34:21 INFO: Loading these models for language: ru (Russian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-06-07 20:34:21 INFO: Use device: cpu\n",
      "2021-06-07 20:34:21 INFO: Loading: tokenize\n",
      "2021-06-07 20:34:21 INFO: Loading: pos\n",
      "2021-06-07 20:34:23 INFO: Loading: lemma\n",
      "2021-06-07 20:34:23 INFO: Loading: depparse\n",
      "2021-06-07 20:34:25 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('uk')\n",
    "stanza.download('ru', package='gsd', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "nlp = stanza.Pipeline('uk', processors='tokenize,lemma')\n",
    "\n",
    "nlp_uk = init_parser(\n",
    "        \"stanza\",\n",
    "        \"uk\", \n",
    "        is_tokenized = True,\n",
    "        include_headers=False,\n",
    "        parser_opts = {'processors': 'tokenize,pos,lemma,depparse'}\n",
    "    )\n",
    "\n",
    "nlp_ru = init_parser(\n",
    "        \"stanza\",\n",
    "        \"ru\", \n",
    "        is_tokenized = True,\n",
    "        include_headers=False,\n",
    "        parser_opts = {'package': 'gsd', 'processors': 'tokenize,pos,lemma,depparse'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tВсе\tвесь\tPRON\tDT\t_\t2\tnsubj\t_\t_\n",
      "2\tпрошло\tпройти\tVERB\tVBC\t_\t0\troot\t_\t_\n",
      "3\tплохо\tплохо\tADV\tRB\t_\t2\tadvmod\t_\t_\n",
      "4\t,\t,\tPUNCT\t,\t_\t8\tpunct\t_\t_\n",
      "5\tзато\tзато\tSCONJ\tIN\t_\t8\tmark\t_\t_\n",
      "6\tмы\tмы\tPRON\tPRP\t_\t8\tnsubj\t_\t_\n",
      "7\tне\tне\tPART\tNEG\t_\t8\tadvmod\t_\t_\n",
      "8\tхотим\tхотеть\tVERB\tVBC\t_\t2\tadvcl\t_\t_\n",
      "9\tпосле\tпосле\tADP\tIN\t_\t10\tcase\t_\t_\n",
      "10\tэтого\tэто\tPRON\tDT\t_\t8\tobl\t_\t_\n",
      "11\tрасстраиваться\tрасстраиваться\tVERB\tVB\t_\t8\txcomp\t_\tSpaceAfter=No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1=\"Все прошло плохо , зато мы не хотим после этого расстраиваться\"\n",
    "doc = nlp_ru(s1)\n",
    "conll = doc._.conll_str\n",
    "print(conll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import six\n",
    "import razdel\n",
    "\n",
    "\n",
    "ACCENT = six.unichr(769)\n",
    "WORD_TOKENIZATION_RULES = re.compile(r\"\"\"\n",
    "[\\w\"\"\" + ACCENT + \"\"\"]+://(?:[a-zA-Z]|[0-9]|[$-_@.&+])+\n",
    "|[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+\n",
    "|[0-9]+-[а-яА-ЯіїІЇєЄґҐ'’`\"\"\" + ACCENT + \"\"\"]+\n",
    "|[+-]?[0-9](?:[0-9,.-]*[0-9])?\n",
    "|[\\w\"\"\" + ACCENT + \"\"\"](?:[\\w'’`-\"\"\" + ACCENT + \"\"\"]?[\\w\"\"\" + ACCENT + \"\"\"]+)*\n",
    "|[\\w\"\"\" + ACCENT + \"\"\"].(?:\\[\\w\"\"\" + ACCENT + \"\"\"].)+[\\w\"\"\" + ACCENT + \"\"\"]?\n",
    "|[\"#$%&*+,/:;<=>@^`~…\\\\(\\\\)⟨⟩{}\\[\\|\\]‒–—―«»“”‘’'№]\n",
    "|[.!?]+\n",
    "|-+\n",
    "\"\"\", re.X | re.U)\n",
    "\n",
    "\n",
    "ABBRS = \"\"\"\n",
    "ім.\n",
    "в.\n",
    "о.\n",
    "т.\n",
    "п.\n",
    "д.\n",
    "под.\n",
    "ін.\n",
    "вул.\n",
    "просп.\n",
    "бул.\n",
    "пров.\n",
    "пл.\n",
    "г.\n",
    "р.\n",
    "див.\n",
    "п.\n",
    "с.\n",
    "м.\n",
    "н.\n",
    "е.\n",
    "адмін.\n",
    "к.\n",
    "геогр.\n",
    "обл.\n",
    "смт.\n",
    "авт.\n",
    "адм.\n",
    "акад.\n",
    "англ.\n",
    "арк.\n",
    "арт.\n",
    "археол.\n",
    "арх.\n",
    "архіт.\n",
    "асист.\n",
    "асоц.\n",
    "б.\n",
    "буд.\n",
    "бух.\n",
    "бюдж.\n",
    "вет.\n",
    "вид.\n",
    "викл.\n",
    "відкр.\n",
    "дип.\n",
    "діагр.\n",
    "екол.\n",
    "екон.\n",
    "євр.\n",
    "журн.\n",
    "зобр.\n",
    "іл.\n",
    "інв.\n",
    "інд.\n",
    "інж.\n",
    "іст.\n",
    "каф.\n",
    "кл.\n",
    "коеф.\n",
    "лаб.\n",
    "лінгв.\n",
    "літ.\n",
    "мат.\n",
    "мед.\n",
    "мех.\n",
    "міс.\n",
    "муз.\n",
    "нар.\n",
    "нац.\n",
    "орг.\n",
    "офіц.\n",
    "пед.\n",
    "пр.\n",
    "проф.\n",
    "публ.\n",
    "рис.\n",
    "мал.\n",
    "pp.\n",
    "рос.\n",
    "св.\n",
    "сл.\n",
    "ст.\n",
    "студ.\n",
    "табл.\n",
    "тис.\n",
    "укр.\n",
    "упр.\n",
    "фіз.\n",
    "фін.\n",
    "ц.\n",
    "\"\"\".strip().split()\n",
    "\n",
    "\n",
    "def tokenize_sents(string):\n",
    "    string = six.text_type(string)\n",
    "    spans = []\n",
    "    for match in re.finditer('[^\\s]+', string):\n",
    "        spans.append(match)\n",
    "    spans_count = len(spans)\n",
    "\n",
    "    rez = []\n",
    "    off = 0\n",
    "\n",
    "    for i in range(spans_count):\n",
    "        tok = string[spans[i].start():spans[i].end()]\n",
    "        if i == spans_count - 1:\n",
    "            rez.append(string[off:spans[i].end()])\n",
    "        elif tok[-1] in ['.', '!', '?', '…', '»', \"'\", \"\\\"\"]:\n",
    "            # tok1 = tok[re.search('[.!?…»]', tok).start() - 1]\n",
    "            next_tok = string[spans[i + 1].start():spans[i + 1].end()]\n",
    "            if (next_tok[0].isupper() or next_tok[0] in [\"'\", \"\\\"\", \"«\"]) \\\n",
    "                    and not ((len(tok) == 2 and tok[0].isupper()) \\\n",
    "                             or tok[0] == '('\n",
    "                             or tok in ABBRS):\n",
    "                rez.append(string[off:spans[i].end()])\n",
    "                off = spans[i + 1].start()\n",
    "\n",
    "    return rez\n",
    "\n",
    "\n",
    "def text_to_sent(text, lang):\n",
    "    rez = []\n",
    "    if lang == 'uk':\n",
    "        for part in text.split('\\n'):\n",
    "            rez += tokenize_sents(part)\n",
    "    elif lang=='ru':\n",
    "        for part in text.split('\\n'):\n",
    "            rez += [s.text for s in razdel.sentenize(part)]\n",
    "    return rez\n",
    "\n",
    "\n",
    "def sent_to_words(text, lang):\n",
    "    if lang == 'uk':\n",
    "        return re.findall(WORD_TOKENIZATION_RULES, text)\n",
    "    elif lang == 'ru':\n",
    "        return [tkn.text for tkn in razdel.tokenize(text)]\n",
    "    return None\n",
    "\n",
    "# def tokenize(text, lang):\n",
    "#     res = []\n",
    "#     for sent in text_to_sent(text, lang):\n",
    "#         tokens = []\n",
    "#         for word in sent_to_words(sent, lang):\n",
    "#             tokens.append(word)\n",
    "#         res.append(tokens)\n",
    "#     return res\n",
    "\n",
    "def tokenize(text, lang):\n",
    "    res = []\n",
    "    for sent in text_to_sent(text, lang):\n",
    "        tokens = []\n",
    "        for word in sent_to_words(sent, lang):\n",
    "            tokens.append(word)\n",
    "        res.append(' '.join(tokens))\n",
    "    return '\\n'.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'text', 'subtitle', 'link', 'domain', 'datetime', 'views',\n",
       "       'created_at', 'category', 'language', 'id', 'domain_alias',\n",
       "       'mycategory', 'mentions', 'hash', 'all_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 14s, sys: 2.7 s, total: 4min 16s\n",
      "Wall time: 4min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "news['tokenized'] = news.apply(lambda row: tokenize(row.all_text, row.language), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_conllu(doc, link_hash, out_dir = '/Users/oksana/Dev/Politicians_in_media/april_conllu/conllu_uk/'):\n",
    "    out_file = out_dir + \"{}.conll\".format(link_hash)\n",
    "    for sent_idx, sent in enumerate(doc.sents, 1):\n",
    "        header = ['### ', sent_idx, link_hash]\n",
    "        pd.DataFrame([header]).to_csv(out_file, sep='\\t', index=False, header=None, mode='a')\n",
    "        sent._.conll_pd.to_csv(out_file, index=False, sep=\"\\t\", encoding='utf-8', mode='a', header=None)\n",
    "        with open(out_file, 'a') as f:\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.drop(columns=['all_text', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_uk = news[news.language=='uk']\n",
    "news_ru = news[news.language=='ru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'link', 'title', 'domain', 'datetime', 'views', 'category',\n",
       "       'language', 'domain_alias', 'found_names_str', 'mycategory', 'mentions',\n",
       "       'tone_words', 'hash', 'tokenized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_uk.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news(news, nlp=nlp_uk, start=0, finish=1000, step=100, out_dir='/Users/oksana/Dev/Politicians_in_media/april_conllu/conllu_uk/'):\n",
    "    for k in range(start, finish, step):\n",
    "        try:\n",
    "            del news_part\n",
    "        except:\n",
    "            pass    \n",
    "        news_part = news.iloc[k:k + step].copy()\n",
    "        news_part = news_part[news_part.mentions.notna()]\n",
    "        news_part['docs'] = news_part.tokenized.apply(nlp)\n",
    "        news_part.apply(lambda row: save_to_conllu(row.docs, row.hash, out_dir), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[['link', 'hash', 'id']].to_csv('link_hash_id_april.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123238, 15)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_uk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 58min 13s, sys: 15min 45s, total: 2h 13min 58s\n",
      "Wall time: 2h 13min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_news(news_uk, nlp_uk, start=100000, finish=len(news_uk), step=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 38min 3s, sys: 35min 5s, total: 5h 13min 9s\n",
      "Wall time: 4h 58min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_news(news_uk, nlp_uk, start=50000, finish=100000, step=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25752    None\n",
       "25753    None\n",
       "25756    None\n",
       "25758    None\n",
       "25765    None\n",
       "         ... \n",
       "26737    None\n",
       "26739    None\n",
       "26740    None\n",
       "26749    None\n",
       "26753    None\n",
       "Length: 217, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_part['docs'] = news_part.tokenized.apply(nlp_uk)\n",
    "news_part.apply(lambda row: save_to_conllu(row.docs, row.hash, '/Users/oksana/Dev/Politicians_in_media/march_conllu/ukr/'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25min 1s, sys: 3min 29s, total: 28min 30s\n",
      "Wall time: 27min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_news(news_uk, nlp_uk, start=80000, finish=85000, step=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = os.listdir('/Users/oksana/Dev/Politicians_in_media/march_conllu/ukr/')\n",
    "len(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis with __[UUUSA](https://github.com/aghie/uuusa)__   ( __[article](https://arxiv.org/abs/1606.05545)__,  __[manual](http://grupolys.org/software/UUUSA/uuusa-user-manual.pdf)__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java -Dfile.encoding=UTF-8 -jar -Xmx2g samulan-0.1.0.jar \\ \n",
    "-s UkSentiData \\\n",
    "-r configuration_uk.xml \\\n",
    "-c parsed.conll \\\n",
    "-p samulan.properties \\\n",
    "-v true\n",
    "-o output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in *.conll; do java -Dfile.encoding=UTF-8 -jar -Xmx2g ../samulan-0.1.1.jar \\\n",
    "-s ../UkSentiData \\\n",
    "-r ../configuration_uk.xml \\\n",
    "-c $file \\\n",
    "-p ../samulan.properties\\\n",
    "-sc so \\\n",
    "-o ../output/$file; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.0\t\tКаменських назвала свій головний привід для радості , і це не Потап : \"\"\"\" Згодні ? \"\"\"\"\t [The analysis took: 0.060065182 seg.] [Accumulated time is: 0.060065182]\n",
    "    3.0\t\tПопулярна українська співачка Настя Каменських , яка минулого року вийшла заміж за свого колегу Олексія Потапенка , часто радує публіку яскравими постами в своєму Instagram .\t [The analysis took: 0.001861094 seg.] [Accumulated time is: 0.061926276]\n",
    "    1.0\t\tНа карантині у зірки з'явилося більше часу для спілкування з фанатами , чим вона активно користується .\t [The analysis took: 0.001455214 seg.] [Accumulated time is: 0.06338149]\n",
    "    0.0\t\tЧитайте Знай в Google News !\t [The analysis took: 7.0612E-4 seg.] [Accumulated time is: 0.06408761\n",
    "    2.0\t\tНещодавно дружина Потапа розмістила свіже фото , на якому позує біля пальми в легкому жіночному образі – жовтих шортах , білій сорочці та верху від купальника теж білого кольору .\t [The analysis took: 0.002728221 seg.] [Accumulated time is: 0.066815831]\n",
    "    1.0\t\tУ підписі Каменських зазначила , що п'ятниця вже є приводом для радості .\t [The analysis took: 0.002728221 seg.] [Accumulated time is: 0.066815831]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_files = os.listdir(sentiment_dir)\n",
    "res = []\n",
    "for file in sentiment_files:\n",
    "    s = pd.read_csv(file, sep='\\t', usecols=[0], header=None)[0]\n",
    "    res.append((file.strip('.conll'), s.sum(), s.astype(str).str.cat(sep=';')))\n",
    "df = pd.DataFrame(res, columns=['hash', 'sentiment', 'sent_list'])\n",
    "df.hash = df.hash.astype('uint64')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
